<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · DRIPs.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">DRIPs.jl</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#A-Structure-for-LQG-Dynamic-Rational-Inattention-Problems-(D.R.I.P.)-1"><span>A Structure for LQG Dynamic Rational Inattention Problems (D.R.I.P.)</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/afrouzi/DRIPs.jl/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="A-Toolbox-for-Solving-Dynamic-Rational-Inattention-Problems-1"><a class="docs-heading-anchor" href="#A-Toolbox-for-Solving-Dynamic-Rational-Inattention-Problems-1">A Toolbox for Solving Dynamic Rational Inattention Problems</a><a class="docs-heading-anchor-permalink" href="#A-Toolbox-for-Solving-Dynamic-Rational-Inattention-Problems-1" title="Permalink"></a></h1><p>This notebook explains how to solve LQG Dynamic Rational Inattention models using the methods and the <a href="https://github.com/choongryulyang/dynamic_multivariate_RI">solver</a> from <a href="http://www.afrouzi.com/dynamic_inattention.pdf">Afrouzi and Yang (2019)</a> that was adapted for Julia by the authors and <a href="https://www.acostamiguel.com/home">Miguel Acosta</a>. </p><p>To add the package, execute:</p><pre><code class="language-none">] DRIPs</code></pre><p>To include the package, execute:</p><pre><code class="language-julia">using DRIP;</code></pre><h2 id="A-Structure-for-LQG-Dynamic-Rational-Inattention-Problems-(D.R.I.P.)-1"><a class="docs-heading-anchor" href="#A-Structure-for-LQG-Dynamic-Rational-Inattention-Problems-(D.R.I.P.)-1">A Structure for LQG Dynamic Rational Inattention Problems (D.R.I.P.)</a><a class="docs-heading-anchor-permalink" href="#A-Structure-for-LQG-Dynamic-Rational-Inattention-Problems-(D.R.I.P.)-1" title="Permalink"></a></h2><h3 id="The-Problem-1"><a class="docs-heading-anchor" href="#The-Problem-1">The Problem</a><a class="docs-heading-anchor-permalink" href="#The-Problem-1" title="Permalink"></a></h3><p>A LQG Dynamic Rational Inattention Problem (<strong>drip</strong>) is defined as the following tracking problem, where at any point in time the agent chooses a vector of actions <span>$\vec{a}_t\in\mathbb{R}^m$</span> to track a Gaussian stochastic process <span>$\vec{x}_t\in\mathbb{R}^n$</span>: \begin{align}     &amp; \min<em>{{\vec{a}</em>t}<em>{t\geq 0}} \mathbb{E}\left[\sum</em>{t=0}^\infty \beta^t (\vec{a}<em>t - \vec{x}</em>t&#39;\mathbf{H})&#39;(\vec{a}<em>t - \vec{x}</em>t&#39;\mathbf{H}) - \omega \mathbb{I}(\vec{a}^t;\vec{x}^t|\vec{a}^{t-1})\lvert \vec{a}^{-1}\right] \
    s.t.\quad &amp;         \vec{x}<em>t=\mathbf{A}\vec{x}</em>{t-1}+\mathbf{Q}\vec{u}<em>t,\quad \vec{u}</em>t\sim \mathcal{N}(\mathbf{0},\mathbf{I}^{k\times k}) \end{align} Here:</p><ul><li><div>\[\vec{a}_t\in\mathbb{R}^m\]</div>is the vector of the agent&#39;s actions at time <span>$t$</span> (a firms choosing a price, or a househld choosing consumption and labor etc.) We denote the number of actions by <span>$m$</span>.</li><li><div>\[\vec{x}_t\in\mathbb{R}^n\]</div>is the vector of the shocks that the agent faces at time <span>$t$</span> that are exogenous to her decision, but could be endogenous to the GE model (marginal cost of a firm, real interest rates etc.) We denote the number of shocks by <span>$n$</span>.</li></ul><h4 id="The-Parameters-1"><a class="docs-heading-anchor" href="#The-Parameters-1">The Parameters</a><a class="docs-heading-anchor-permalink" href="#The-Parameters-1" title="Permalink"></a></h4><p>The LQG-DRI problem is characterized by the following parameters:</p><ul><li><div>\[\omega\in \mathbb{R}_+\]</div>: cost of 1 bit of information in units of the agent&#39;s payoff. </li><li><div>\[\beta\in[0,1]\]</div>: rate of discounting information.</li><li><div>\[\mathbf{A}\in \mathbb{R}^{n\times n}, \mathbf{Q}\in\mathbb{R}^{n\times k}\]</div>: Determine the state space representation of <span>$\vec{x}_t$</span>.</li><li><div>\[\mathbf{H}\in \mathbb{R}^{n\times m}\]</div>: interaction of payoffs with shocks. This comes from a second order approximation to the utility function and is such that under full information <span>$\vec{a}^*=\mathbf{H}&#39;\vec{x}$</span>.</li></ul><h4 id="The-Solution-1"><a class="docs-heading-anchor" href="#The-Solution-1">The Solution</a><a class="docs-heading-anchor-permalink" href="#The-Solution-1" title="Permalink"></a></h4><p>The solution to the dynamic rational inattention problem is a joint stochastic process between the actions and the states: <span>$\{(\vec{a}_t,\vec{x}_t):t\geq 0\}$</span>. Moreover, in some economic applications, we are also interested in the law of motion for the agent&#39;s belief about <span>$\vec{x}_t$</span> under the optimal information structure <span>$\hat{x}_t=\mathbb{E}_t[\vec{x}_t]$</span> where the expectation is taken conditional on the agent&#39;s time <span>$t$</span> information.</p><p>Theorem 2 and Proposition 3 in <a href="http://www.afrouzi.com/dynamic_inattention.pdf">Afrouzi and Yang (2019)</a> characterize this joint distribution as a function of a tuple <span>$(\mathbf{K},\mathbf{Y},\mathbf{\Sigma}_z)$</span> where \begin{align}     \vec{a}<em>t &amp;= \mathbf{H}&#39;\hat{x}</em>t = \mathbf{H}&#39;\mathbf{A}\hat{x}<em>{t-1}+\mathbf{Y}&#39;(\vec{x}</em>t-\mathbf{A}\hat{x}<em>{t-1})+\vec{z}</em>t \
    \hat{x}<em>t &amp;= \mathbf{A}\hat{x}</em>{t-1}+\mathbf{K}\mathbf{Y}&#39;(\vec{x}<em>t-\mathbf{A}\hat{x}</em>{t-1})+\mathbf{K}\vec{z}<em>t,\quad \vec{z}\sim\mathcal{N}(0,\Sigma</em>z) \end{align}</p><p>Here, </p><ul><li><div>\[\mathbf{K}\in\mathbb{R}^{n\times m}\]</div>is the Kalman-gain matrix of the agent in under optimal information acquisition in the stationary solution.</li><li><div>\[\mathbf{Y}\in\mathbb{R}^{m\times m}\]</div>maps the state to the agent&#39;s action under rational inattetion and govern the covariance of the two.</li><li><div>\[\mathbf{\Sigma}_z\in\mathbb{R}^{m\times m}\]</div>is the variance-covariance matrix of the agent&#39;s rational inattention error.</li></ul><p>In addition to these, we might also be interested in the agent&#39;s prior and posterior subjective undertainty, along with the continution value that she assigns to information:</p><ul><li><div>\[\mathbf{\Sigma}_{p}=\mathbb{V}ar(\vec{x}_t|\vec{a}^{t})\in\mathbb{R}^{n\times n}\]</div>.</li><li><div>\[\mathbf{\Sigma}_{-1}=\mathbb{V}ar(\vec{x}_t|\vec{a}^{t-1})\in\mathbb{R}^{n\times n}\]</div>, </li><li><div>\[\bar{\Omega}\in\mathbb{R}^{n\times n}\]</div>.</li></ul><p>The matrix <span>$\bar{\Omega}$</span>, which is the continuation value of information, is also important for the number of iterations that the code needs to converge. When <span>$n$</span> is large, accurate guesses for <span>$\bar{\Omega}$</span> and <span>$\mathbf{\Sigma}_{-1}$</span> speed up the convergence considerably.</p><h3 id="The-Solver-for-the-Steady-State-of-D.R.I.P.-1"><a class="docs-heading-anchor" href="#The-Solver-for-the-Steady-State-of-D.R.I.P.-1">The Solver for the Steady State of D.R.I.P.</a><a class="docs-heading-anchor-permalink" href="#The-Solver-for-the-Steady-State-of-D.R.I.P.-1" title="Permalink"></a></h3><p>The solver function is <code>solve_drip(ω,β,A,Q,H;Σ0 = A*A&#39;+Q*Q&#39;,Ω0 = H*H&#39;)</code>. It takes the primitives <code>(ω,β,A,Q,H;Σ0,Ω0)</code> as arguments, where the arguments <code>Σ0,Ω0</code> are the initial guesses for <span>$\mathbf{\Sigma}_{-1}$</span> and <span>$\bar{{\Omega}}$</span> and are optional for cases in which an accurate initial guess is at hand. If not specified, the default values are set as <code>Ω0=H*H&#39;</code> and <code>Σ0=Q*Q&#39;</code>. See the code for extra options regarding tolerance of convergence, maximum number of iterations, and updating weight for every iteration.</p><p>The function returns the solution of the model within a <code>drip</code> structure,  that contains all the primitives and the solution objects of the model:</p><pre><code class="language-julia">mutable struct Drip 
    ω; β; A; Q; H;                    # primitives
    K; Y; Σ_z; Σ_p; Σ_1; Ω;           # solution
    Drip()          = new();
    Drip(ω,β,A,Q,H) = new(ω,β,A,Q,H);
end</code></pre><h3 id="Impulse-Resonse-Functions-using-Steady-State-Information-Structure-1"><a class="docs-heading-anchor" href="#Impulse-Resonse-Functions-using-Steady-State-Information-Structure-1">Impulse Resonse Functions using Steady State Information Structure</a><a class="docs-heading-anchor-permalink" href="#Impulse-Resonse-Functions-using-Steady-State-Information-Structure-1" title="Permalink"></a></h3><p>Once the model is solved, one can generate the impulse response functions of actions and beliefs using the laws of motion stated above. </p><p>We have also included a built-in function that generates these IRFs. The function <code>dripirfs(P::drip,T::Int)</code> takes a <code>drip</code> structure as input along with the length of irfs <code>T</code> and returns the irfs of the state, beliefs and actions to all possible shocks in the following structure:</p><pre><code class="language-julia">struct Dripirfs
    T     :: Int            # length of IRFs
    x     :: Array{Float64} # IRFs of the fundamental shocks
    x_hat :: Array{Float64} # IRFs of beliefs
    a     :: Array{Float64} # IRFs of actions
end</code></pre><p>Here </p><ul><li><p><code>x</code> is of dimension <span>$n\times k \times T$</span> where <code>x[i,j,:]</code> is the IRF of the <span>$i$</span>&#39;th element of the state vector (<span>$\vec{x}_t$</span>) to the <span>$j$</span>&#39;th shock in the vector <span>$\vec{u}_t$</span>.</p></li><li><p><code>x_hat</code> is of dimension <span>$n\times k \times T$</span> where <code>x_hat[i,j,:]</code> is the IRF of the agent&#39;s belief about <span>$i$</span>&#39;th element of the state vector  (<span>$\hat{x}_t$</span>) to the <span>$j$</span>&#39;th shock in the vector <span>$\vec{u}_t$</span>.</p></li><li><p><code>x_hat</code> is of dimension <span>$m\times k \times T$</span> where <code>a[i,j,:]</code> is the IRF of the <span>$i$</span>&#39;th element of the action vector (<span>$\vec{a}_t$</span>) to the <span>$j$</span>&#39;th shock in the vector <span>$\vec{u}_t$</span>.</p></li></ul><h3 id="The-Solver-for-the-Transition-dynamics-of-Rational-Inattention-Problems-(T.R.I.P.)-1"><a class="docs-heading-anchor" href="#The-Solver-for-the-Transition-dynamics-of-Rational-Inattention-Problems-(T.R.I.P.)-1">The Solver for the Transition dynamics of Rational Inattention Problems (T.R.I.P.)</a><a class="docs-heading-anchor-permalink" href="#The-Solver-for-the-Transition-dynamics-of-Rational-Inattention-Problems-(T.R.I.P.)-1" title="Permalink"></a></h3><p>The Euler equation derived in <a href="http://www.afrouzi.com/dynamic_inattention.pdf">Afrouzi and Yang (2019)</a> for the  also allows us to characterize the transition path of the information structure over time for an arbitrary initial prior. The function <code>solve_trip(P::drip; T=100)</code> takes the solution <code>P=solve_drip(.)</code> as an input and returns a <code>trip</code> structure that summarizes the transition path of information benefit matrics, priors and a vector for the marginal values of information over time (the input <code>T</code> determines the number of periods that it takes to reach to the steady state and is set to 100 by default):</p><pre><code class="language-julia">struct Trip 
    P::Drip;                # problem and solution in steady state
    T::Int;                 # length of T.R.I.P.
    Σ_1s; Σ_ps; Ωs;         # priors, posteriors and benefit matrices 
    Ds;                     # eigenvalues of Σ_t^(0.5)Ω_tΣ_t^(0.5)
end </code></pre></article></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 30 April 2020 08:20">Thursday 30 April 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
